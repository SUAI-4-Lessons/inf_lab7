#import "template.typ": *

#show: init

#ch([Введение])

В данном отчёте представлено выполнение лабораторной работы №7. Основной целью
данной работы являются получение практических навыков численного определения
количества информации, содержащегося в сообщении, а также освоение методов
построения кодов дискретного источника информации, используя конструктивный
метод, предложенный К. Шенноном и Н. Фано, и метод Хаффмана. Также в рамках
работы на примере демонстрируется однозначность раскодирования имеющегося
сообщения.

Лабораторная работа состоит из 3 частей, выполняемых последовательно и
оцениваемых отдельно.

#pagebreak()

= Определение количества информации

Использованный текст: Джон Рональд Руэл Толкин. Властелин Колец Две Крепости: ISBN:978–5–17–89238–9. АСТ. 384 с. .

Создана программа на языке программирования Kotlin с использованием фреймворка Compose Multiplatform для расчёта значений
необходимых для данной лабораторной работы. Исходный код программы прикреплён к отчёту.

Таблица расчёта энтропии источника представлена в таблице @entropy_table.

#let entropy = csv("tables/entropy.csv")

#figure(table(
  columns: 6,
  table.header(..entropy.remove(0).flatten()),
  ..entropy.flatten(),
), caption: [Расчёт энтропии ]) <entropy_table>

Формулы использованные для расчёта:

- вероятность вхождения символа в текст по формуле @prob;

  $
    p_i = k_i/K
  $ <prob>

  где:

  $k_i$ -- количество символов в тексте,

  $K$ -- количество всех символов;
- количество информации для каждого символа по формуле @entropy

  $
    I_i = - log_2 p_i space space space p_i != 0
  $ <entropy>

- Энтропия источника по формуле

  $
    H = I_"ср" = sum p_i dot I_i
  $

На рисунке представлен рисунок из созданной программы:

#figure(
  image("images/1.png", width: 50%),
  caption: [Таблица энтропии из программы],
)

В таблице @codings представлено сравнение кодировки ASCII и использование
равномерного кода на основе Хартли.

#let codings = csv("tables/codings.csv")

#figure(table(
  columns: 5,
  table.header(..codings.remove(0).flatten()),
  ..codings.flatten(),
), caption: [Расчёт энтропии ]) <codings>

#h(-0.2em)Для расчёта данной таблицы @codings использовались следующие формулы:

- Неопределённость (или энтропия) равномерного источника:

  $
    H_"max" = log_2 A
  $

  где:

  $A$ -- количество символов в алфавите

  Для ASCII-кода: $log_2 128$ = 7 бит

  Для равномерного кода на основе меры Хартли:

  $log_2 50 approx 5.64$
- Разрядность кода по формуле @n

  $
    n = ceil(log_2 A) = ceil(H_"max")
  $ <n>

  #h(1.25cm) Для ASCII-кода: 8 бит

  Для равномерного кода на основе меры Хартли: $ceil(5.64) = 6$

- Абсолютная избыточность по формуле @D_abc

  $
    D_"abc" = H_max - H
  $ <D_abc>

  #h(1.25cm) Для ASCII-кода: $7 - 4.5080266 approx 2.49$

  Для равномерного кода Хартли:

  $5.643856 - 4.5080266 approx 1.14$

- Относительная избыточность по формуле @D

  $
    D = (D_"abc") / H_max = (H_max - H) / H_max
  $ <D>

  #h(1.25cm) Для ASCII-кода: $2.49/7 approx 0.36$

  Для равномерного кода Хартли: $1.14/5.64 approx 0.2$

== Вывод по сравнению кодировок

Максимальная энтропия $H_max$​ для ASCII-кода составляет 7 бит, что заметно
превышает аналогичный показатель равномерного кода Хартли (5.64 бита) и
обусловлено более крупным алфавитом. Для представления символов ASCII требуется
8 бит, тогда как равномерному коду Хартли достаточно всего 6. Абсолютная
избыточность ASCII достигает 2.49 бита, в то время как у кода Хартли этот
показатель равен лишь 1.14 бита. Относительная избыточность также отличается: у
ASCII-кода она составляет 36%, что выше, чем 20% у равномерного кода Хартли, что
указывает на меньшую эффективность использования битов. Таким образом,
равномерный код Хартли оказывается более экономичным и рациональным благодаря
снижению избыточности.

#pagebreak()

= Кодирование дискретных источников информации методом Шеннона-Фано

На рисунке @fano представлена вычисление кода Шеннона-Фано с помощью написанного
приложения.

#figure(
  image("images/2.png"),
  caption: "Отображение сгенерированного кода в приложении",
) <fano>

Для наглядного отображения данных таблица сгенерированного кода по алгоритму
Шеннона-Фано представлена в таблице

#let shenon = csv("tables/shanon.csv")

#figure(table(
  columns: 3,
  table.header(..shenon.remove(0).flatten()),
  ..shenon.flatten(),
), caption: [Код сгенерированный по алгоритму Шенона-Фано])

Энтропия источника осталось неизменной и равной 4.51.
Рассчитаем среднее количество двоичных разряд. Рассчёт среднего количества
двоичных разрядов представлен в формуле @avg_shit.

$
  n_"ср" = sum_(i = 1)^50 p_i a_i approx 4.62
$ <avg_shit>

== Пример декодирования сообщения

Закодируем простое сообщение "панков м412". В данном случае воспользуюсь
разработанной программой и получу код представленный на рисунке @fuck1.

#figure(image("images/3.png"), caption: "Сгенерированный код") <fuck1>



Получив сообщение подобного вида, необходимо его декодировать, чтобы прочитать. Считаем, что получатель имеет таблицу кодировки символов, идентичную с отправителем. Декодирование производится с наименьшей длины кодового слова, — получается таблица @decode1 с итоговым результатом, аналогичным закодированному

#let decodeShanon = csv("tables/decode_sh.csv")

#figure(
  table(columns: 4, table.header(..decodeShanon.remove(0).flatten()),
  ..decodeShanon.flatten()),
  caption: [Декодирование кода],
) <decode1>



== Вывод

В ходе практического занятия изучался конструктивный подход к созданию кодов для дискретного источника информации, разработанный Клодом Шенноном. Была продемонстрирована работа основной теоремы Шеннона. Созданный код оказался близким к максимально эффективному решению по Шеннону, хотя и не достигал оптимальности. На конкретном примере было показано, что раскодирование переданного сообщения возможно однозначно.


= Кодирование дискретных источников информации методом Д.Хаффмана

На рисунке @huffman представлен скриншот из разработанного приложения со сложением вероятностей на каждом шаге.

#figure(image("images/4.png"), caption: [Посимвольное кодирование методом Хаффмана])<huffman>

По итогу получилась таблица @huff.

#let huffman = csv("tables/huffman.csv")

#figure(table(columns: 3, table.header(..huffman.remove(0).flatten()),
  ..huffman.flatten()),
  caption: [Таблица кодировки по Хаффману],) <huff>

Кодовое дерево представлено на рисунке @hgraf.

#figure(image("images/6.png"), caption: [Построенный граф]) <hgraf>


Изображение графа для подробного просмотра прикреплено к отчёту.

Энтропия источника осталось неизменной и равной 4.51.
Рассчитаем среднее количество двоичных разряд. Рассчёт среднего количества
двоичных разрядов представлен в формуле @avg_shit2.

$
  n_"ср" = sum_(i = 1)^50 p_i a_i approx 4.54
$ <avg_shit2>

== Пример декодирования сообщения

Закодируем простое сообщение "панков м412". В данном случае воспользуюсь
разработанной программой и получу код представленный на рисунке @fuck2.

#figure(image("images/5.png"), caption: "Сгенерированный код") <fuck2>

Получив сообщение подобного вида, необходимо его декодировать, чтобы прочитать. Считаем, что получатель имеет таблицу кодировки символов, идентичную с отправителем. Декодирование производится с наименьшей длины кодового слова, — получается таблица @decode2 с итоговым результатом, аналогичным закодированному

#let decodeHuffman = csv("tables/decode_h.csv")

#figure(
  table(columns: 4, table.header(..decodeHuffman.remove(0).flatten()),
  ..decodeHuffman.flatten()),
  caption: [Декодирование кода],
) <decode2>


== Вывод

На практике был изучен и применён метод кодирования дискретных источников информации, разработанный Д. Хаффманом. В отличие от рассмотренного ранее метода, подверженного неоднозначности, алгоритм Хаффмана обеспечивает однозначное кодирование. Этот подход позволяет создавать код с минимально возможным средним количеством двоичных символов для заданного распределения вероятностей. 

В процессе построения кодового дерева установлено, что кодовые слова присваиваются исключительно конечным вершинам дерева. Благодаря этому достигается важное свойство: ни одно кодовое слово не является началом другого, что делает возможным точное разделение последовательности на отдельные элементы.

На примере было подтверждено, что декодирование сообщений, закодированных методом Хаффмана, является однозначным.

#pagebreak()

#ch[ЗАКЛЮЧЕНИЕ]

В ходе выполнения лабораторной работы №7 были приобретены практические навыки по численному определению количества информации в сообщении, а также освоены методы построения кодов дискретного источника информации с использованием подходов Шеннона-Фано и Хаффмана. Результаты работы подтвердили теоретическую однозначность декодирования сообщений, что продемонстрировано на конкретных примерах. Последовательное выполнение всех трёх частей работы позволило глубже понять принципы кодирования и декодирования информации, а также закрепить полученные знания на практике.

// #pagebreak()
// #ch[Список Источников]

//#bibliography(
//  "bibliography.yml",
//  title: none,
//  full: true,
//  style: "gost-r-705-2008-numeric",
//)

