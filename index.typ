#import "template.typ": *

#show: init

#ch([Введение])

В данном отчёте представлено выполнение лабораторной работы №7. Основной целью данной работы являются получение практических навыков численного определения количества информации, содержащегося в сообщении, а также освоение методов построения кодов дискретного источника информации, используя конструктивный метод, предложенный К. Шенноном и Н. Фано, и метод Хаффмана. Также в рамках работы на примере демонстрируется однозначность раскодирования имеющегося сообщения.

Лабораторная работа состоит из 3 частей, выполняемых последовательно и оцениваемых отдельно.

#pagebreak()

= Определение количества информации

Использованный текст: Толкин Джон Рональд Руэл "Властелин Колец. Две Крепости"
@dve-bashni.

Создана программа на языке программирования Kotlin для расчёта значений
необходимых для данной лабораторной работы. Исходный код программы представлен в
приложении А.

Таблица расчёта энтропии источника представлена в приложении А.

#let entropy = csv("tables/entropy.csv")

#figure(table(
  columns: 6,
  table.header(..entropy.remove(0).flatten()),
  ..entropy.flatten(),
), caption: [Расчёт энтропии ])

Формулы использованные для расчёта:

- вероятность вхождения символа в текст по формуле @prob;

  $
    p_i = k_i/K
  $ <prob>

  где:

  $k_i$ -- количество символов в тексте,

  $K$ -- количество всех символов;
- количество информации для каждого символа по формуле @entropy

  $
    I_i = - log_2 p_i space space space p_i != 0
  $ <entropy>

- Энтропия источника по формуле

  $
    H = I_"ср" = sum p_i dot I_i
  $

На рисунке представлен рисунок из созданной программы:

#figure(
  image("images/1.png", width: 50%),
  caption: [Таблица энтропии из программы],
)

В таблице @codings представлено сравнение кодировки ASCII и использование
равномерного кода на основе Хартли.

#let codings = csv("tables/codings.csv")

#figure(table(
  columns: 5,
  table.header(..codings.remove(0).flatten()),
  ..codings.flatten(),
), caption: [Расчёт энтропии ]) <codings>


#h(-0.2em)Для расчёта данной таблицы @codings использовались следующие формулы:

- Неопределённость (или энтропия) равномерного источника:

  $
    H_"max" = log_2 A
  $

  где:

  $A$ -- количество символов в алфавите

  Для ASCII-кода: $log_2 128$ = 7 бит

  Для равномерного кода на основе меры Хартли:

  $log_2 50 approx 5.64$
- Разрядность кода по формуле @n

  $
    n = ceil(log_2 A) = ceil(H_"max")
  $ <n>

  #h(1.25cm) Для ASCII-кода: 8 бит

  Для равномерного кода на основе меры Хартли: $ceil(5.64) = 6$

- Абсолютная избыточность по формуле @D_abc

  $
    D_"abc" = H_max - H
  $ <D_abc>

  #h(1.25cm) Для ASCII-кода: $7 - 4.5080266 approx 2.49$

  Для равномерного кода Хартли:

  $5.643856 - 4.5080266 approx 1.14$

- Относительная избыточность по формуле @D

  $
    D = (D_"abc") / H_max = (H_max - H) / H_max
  $ <D>

  #h(1.25cm) Для ASCII-кода: $2.49/7 approx 0.36$

  Для равномерного кода Хартли: $1.14/5.64 approx 0.2$


== Вывод по сравнению кодировок

Максимальная энтропия $H_max$​ для ASCII-кода составляет 7 бит, что заметно превышает аналогичный показатель равномерного кода Хартли (5.64 бита) и обусловлено более крупным алфавитом. Для представления символов ASCII требуется 8 бит, тогда как равномерному коду Хартли достаточно всего 6. Абсолютная избыточность ASCII достигает 2.49 бита, в то время как у кода Хартли этот показатель равен лишь 1.14 бита. Относительная избыточность также отличается: у ASCII-кода она составляет 36%, что выше, чем 20% у равномерного кода Хартли, что указывает на меньшую эффективность использования битов. Таким образом, равномерный код Хартли оказывается более экономичным и рациональным благодаря снижению избыточности.

#pagebreak()

= Кодирование дискретных источников информации методом Шеннона-Фано

На рисунке @fano представлена вычисление кода Шеннона-Фано с помощью написанного приложения.

#figure(image("images/2.png"), caption: "Отображение сгенерированного кода в приложении") <fano>


Для наглядного отображения данных таблица сгенерированного кода по алгоритму Шеннона-Фано представлена в таблице


#let shenon = csv("tables/shanon.csv")



#figure(table(columns: 3,   table.header(..shenon.remove(0).flatten()),
  ..shenon.flatten(),), caption: [Код сгенерированный по алгоритму Шенона-Фано])

Энтропия источника осталось неизменной, поэтому её рассчитывать не будем. Рассчитаем среднее количество двоичных разряд. Рассчёт среднего количества двоичных разрядов представлен в формуле @avg_shit. 

$
n_"ср" = sum_(i = 1)^50 p_i a_i approx 4.62
$ <avg_shit>


== Пример декодирования сообщения

Закодируем простое сообщение "панков м412". В данном случае воспользуюсь разработанной программой и получу код представленный на рисунке .



#pagebreak()

#ch[Список Источников]

#bibliography(
  "bibliography.yml",
  title: none,
  full: true,
  style: "gost-r-705-2008-numeric",
)

